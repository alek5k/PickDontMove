{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import datetime\n",
    "\n",
    "## V1: Using the full action space\n",
    "## V2: Simplified model\n",
    "#from SimioEnv_v2 import SimioPickDontMoveEnv\n",
    "from SimioEnv_v2p1 import SimioPickDontMoveEnv\n",
    "## TF2\n",
    "from FunctionApproximators_TF2 import ValueEstimator, PolicyEstimator\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from gym_helpers import flatten_space_sample\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "#from lib.envs.cliff_walking import CliffWalkingEnv\n",
    "from lib import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "TF Eager execution active: True\n"
     ]
    }
   ],
   "source": [
    "## TF2\n",
    "\n",
    "####################\n",
    "## RUNNING ON GPU ##\n",
    "####################\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "#####################\n",
    "## EAGER EXECUTION ##\n",
    "#####################\n",
    "# # tf.compat.v1.disable_eager_execution()\n",
    "print(\"TF Eager execution active:\", tf.executing_eagerly()) # https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
    "# without building graphs: operations return concrete values instead of constructing a computational graph to run later.\n",
    "# This makes it easy to get started with TensorFlow and debug models, and it reduces boilerplate as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Pickers:  1\n",
      "Num AGVs:  2\n",
      "Num Warehouse Locations:  8\n",
      "\n",
      "Action Space:\n",
      "=============\n",
      "MultiDiscrete([3 8 8])\n",
      "\n",
      "Observation Space:\n",
      "==================\n",
      "MultiDiscrete([11 11 11 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20])\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('FrozenLake8x8-v0') # FrozenLake8x8-v0 FrozenLake-v0\n",
    "# env = SimioFrozenLakeEnv(num_states=64, num_actions=4, log_output=False)\n",
    "env = SimioPickDontMoveEnv(\n",
    "    num_locations=8, \n",
    "    num_pickers=1, \n",
    "    num_agvs=2,\n",
    "    log_output=False, \n",
    "    log_end_episode_only=False\n",
    "    )\n",
    "\n",
    "print(\"Num Pickers: \", env.num_pickers)\n",
    "print(\"Num AGVs: \", env.num_agvs)\n",
    "print(\"Num Warehouse Locations: \", env.num_locations)\n",
    "\n",
    "print()\n",
    "print(\"Action Space:\")\n",
    "print(\"=============\")\n",
    "print(env.action_space)\n",
    "print()\n",
    "print(\"Observation Space:\")\n",
    "print(\"==================\")\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshapes a list of integers into a format tensorflow can understand\n",
    "def reshape_state(state):\n",
    "    return np.array(state).reshape(-1, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Actor Critic Algorithm. Optimizes the policy function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized \n",
    "        estimator_value: Value function approximator, used as a critic\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    stats = plotting.EpisodeStats(episode_lengths=np.zeros(num_episodes), episode_rewards=np.zeros(num_episodes)) # Keeps track of useful statistics\n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        state = env.reset()\n",
    "        state = reshape_state(state)\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            action_probs = estimator_policy.predict(state)\n",
    "            # action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            \n",
    "            \n",
    "            #TF1\n",
    "            # actions = [np.random.choice(np.arange(len(prob)), p=prob) for prob in action_probs] # action for each picker, agv\n",
    "            \n",
    "            \n",
    "            #TF2\n",
    "            action_probs_flattened = [x.flatten() for x in action_probs]\n",
    "            actions = [np.random.choice(np.arange(len(prob)), p=prob) for prob in action_probs_flattened]\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(actions) \n",
    "            next_state = reshape_state(next_state)\n",
    "            \n",
    "            \n",
    "            # Keep track of the transition, update statistics\n",
    "            episode.append(Transition(state=state, action=actions, reward=reward, next_state=next_state, done=done))\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Calculate TD Target\n",
    "            value_next = estimator_value.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - estimator_value.predict(state)\n",
    "            \n",
    "            # TF1\n",
    "            # # Update the value estimator\n",
    "            value_loss = estimator_value.update(state, td_target)\n",
    "            # # Update the policy estimator\n",
    "            # # using the td error as our advantage estimate\n",
    "            policy_loss = estimator_policy.update(state, td_error, actions)\n",
    "            \n",
    "            # TF2\n",
    "            # Update the value estimator\n",
    "            value_loss = estimator_value.update(state, td_target)\n",
    "            # Update the policy estimator\n",
    "            # using the td error as our advantage estimate\n",
    "            policy_loss = estimator_policy.update(state, td_error, actions)   \n",
    "            policy_loss = policy_loss[0]\n",
    "            \n",
    "            \n",
    "            # print(\"\\r[step {}][ep {}/{}] Vn={} TD={} TDE={} r={} Vloss={} Ploss={}\".format(\n",
    "            #      t,\n",
    "            #      i_episode+1,\n",
    "            #      num_episodes,\n",
    "            #      \n",
    "            #      value_next,\n",
    "            #      td_target,\n",
    "            #      td_error,\n",
    "            #      reward,\n",
    "            #      value_loss,\n",
    "            #      policy_loss), end=\"\")\n",
    "            print(\"\\r\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end=\"\")\n",
    "            print(\"\\r[step {}][ep {}/{}] cumulative_reward={:.2f} Vloss={} Ploss={}\".format(\n",
    "                 t,\n",
    "                 i_episode+1,\n",
    "                 num_episodes,\n",
    "                 stats.episode_rewards[i_episode],\n",
    "                 value_loss,\n",
    "                 policy_loss), end=\"\")\n",
    "            \n",
    "            # probs_string = [\"{:.2f}\".format(x) for x in action_probs[1]]\n",
    "            # print(\"\\r[step {}][ep {}/{}] action_probs={}\".format(\n",
    "            #     t, \n",
    "            #     i_episode+1, \n",
    "            #     num_episodes, \n",
    "            #     probs_string), end=\"\")\n",
    "            \n",
    "            # if td_error > 0.001 or td_error < -0.001:\n",
    "            #     variables = tf.trainable_variables()\n",
    "            #     variables_vals = sess.run(variables)\n",
    "            #     varlist = []\n",
    "            #     for var, val in zip(variables, variables_vals):\n",
    "            #         print(\"var: {}, value: {}\".format(var.name, val))\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            # print(\"\\rStep {} @ Episode {}/{} ({})\\t\\t\\t\\t\\t\".format(t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "            # if td_target > 0.0001 or td_target < -0.0001:\n",
    "            #     print(\"============================\")\n",
    "            #     print(\"Step {} @ Episode {}/{} ({})\".format(t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]))\n",
    "            #     print(\"td target\", td_target, \"td error\", td_error)\n",
    "            #     print(\"value loss\", value_loss, \"policy loss\", policy_loss)\n",
    "            #     print(\"============================\")\n",
    "            # print(\"\\rStep {} @ Episode {}/{} ({})\\t\\t\\t\\t\\t\".format(t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        env.finalize()\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"PolicyEstimator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 19)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30)           600         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            93          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8)            248         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            248         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,189\n",
      "Trainable params: 1,189\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"ValueEstimator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 19)]              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                600       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 631\n",
      "Trainable params: 631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size = len(flatten_space_sample(env.observation_space.sample()))\n",
    "input_shape = (input_size,)\n",
    "# \n",
    "policy_estimator = PolicyEstimator(input_shape, env.picker_actions, env.agv_actions, learning_rate=0.00001)\n",
    "value_estimator = ValueEstimator(input_shape, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%load_ext tensorboard.notebook\n",
    "# %reload_ext tensorboard.notebook\n",
    "# logs_path = \"./tensorboard_actorcriticsimio111\"\n",
    "# summary_writer = tf.summary.FileWriter(logdir=logs_path, graph=tf.get_default_graph()) # , graph=g\n",
    "# %tensorboard --logdir tensorboard_actorcriticsimio111/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "started run: 2020-01-07 16:23:54.770285\n",
      "#######################################\n",
      "WARNING:tensorflow:From C:\\Users\\krnjaia\\AppData\\Local\\Continuum\\anaconda3\\envs\\RL_TFlow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "[step 25][ep 2/200] cumulative_reward=0.00 Vloss=0.008726099506020546 Ploss=0.251715481281280528377234"
     ]
    }
   ],
   "source": [
    "## TF2\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "print(\"#######################################\")\n",
    "print(\"started run:\", datetime.datetime.now())\n",
    "print(\"#######################################\")\n",
    "stats = actor_critic(env, policy_estimator, value_estimator, num_episodes=200, discount_factor=1.0)\n",
    "print(\"######################################\")\n",
    "print(\"ended run:\", datetime.datetime.now())\n",
    "print(\"######################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats, smoothing_window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

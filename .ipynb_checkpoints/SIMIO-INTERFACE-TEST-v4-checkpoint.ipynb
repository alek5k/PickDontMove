{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import zmq\n",
    "import json\n",
    "import random\n",
    "from SimioEnv import SimioPickDontMoveSimplifiedEnv\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from gym_helpers import flatten_space_sample\n",
    "# import tensorflow as tf\n",
    "\n",
    "from IPython.core.debugger import set_trace # https://medium.com/@chrieke/jupyter-tips-and-tricks-994fdddb2057"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Simio Interface Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = SimioPickDontMoveSimplifiedEnv(log_output=False, log_end_episode_only=False)\n",
    "\n",
    "print(\"Num Pickers: \", env.num_pickers)\n",
    "print(\"Num AGVs: \", env.num_agvs)\n",
    "print(\"Num Warehouse Locations: \", env.num_locations)\n",
    "\n",
    "print()\n",
    "print(\"Action Space:\")\n",
    "print(\"=============\")\n",
    "print(env.action_space)\n",
    "print()\n",
    "print(\"Observation Space:\")\n",
    "print(\"==================\")\n",
    "for statevar in env.observation_space:\n",
    "    print(statevar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Picker Observation Space:\", len(flatten_space_sample(env.picker_observation_space.sample())))\n",
    "print(\"Picker Action Space:\", len(flatten_space_sample(env.picker_action_space.sample())))\n",
    "print(\"AGV Observation Space:\", len(flatten_space_sample(env.agv_observation_space.sample())))\n",
    "#print(\"AGV Action Space:\", len(flatten_space_sample(env.agv_action_space.sample())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for i in range(100000):\n",
    "    next_state, reward, done, _ = env.steprandom(log=True)\n",
    "env.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from FunctionApproximators import PolicyEstimator, ValueEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "policy_estimator = PolicyEstimator(env)\n",
    "value_estimator = ValueEstimator(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext tensorboard.notebook\n",
    "#%reload_ext tensorboard.notebook\n",
    "logs_path = \"./tensorboard_pickdontmovetest18\"\n",
    "summary_writer = tf.summary.FileWriter(logdir=logs_path, graph=tf.get_default_graph()) # , graph=g\n",
    "%tensorboard --logdir tensorboard_pickdontmovetest18/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    sample = \n",
    "    value_next = value_estimator.predict(sample)\n",
    "    action_probs = policy_estimator.predict(sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "value_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chosen_actions = []\n",
    "for prob in action_probs:\n",
    "    action = np.random.choice(np.arange(len(prob)), p=prob)\n",
    "    chosen_actions.append(action)\n",
    "\n",
    "ca = np.array([np.random.choice(np.arange(len(prob)), p=prob) for prob in action_probs])\n",
    "print(chosen_actions)\n",
    "print(np.array(chosen_actions))\n",
    "print(ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    # sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    reward = 0.1\n",
    "    discount_factor = 1.0\n",
    "\n",
    "    td_target = reward + discount_factor * value_next\n",
    "    td_error = td_target - value_next \n",
    "\n",
    "    # Update the value estimator\n",
    "    value_loss = value_estimator.update(sample, td_target)\n",
    "    print(value_loss)\n",
    "    \n",
    "    # Update the policy estimator\n",
    "    # using the td error as our advantage estimate\n",
    "    policy_loss = policy_estimator.update(sample, td_error, chosen_actions)\n",
    "    print(policy_loss)\n",
    "    print(chosen_actions)\n",
    "    print([int(a) for a in chosen_actions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(chosen_actions) == len(env.action_space.nvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
